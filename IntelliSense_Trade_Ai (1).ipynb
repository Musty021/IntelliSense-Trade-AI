{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VR-_-jdMCytr"
      },
      "source": [
        "## STEP 1: Setup & Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "57slL7nGg5Rn"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# STEP 1: SETUP & CONFIG\n",
        "# =========================\n",
        "import subprocess\n",
        "import sys\n",
        "import importlib\n",
        "import logging\n",
        "from pathlib import Path\n",
        "\n",
        "# Create project folders\n",
        "Path(\"advanced_ta\").mkdir(exist_ok=True)\n",
        "Path(\"news_sentiment\").mkdir(exist_ok=True)\n",
        "Path(\"cache\").mkdir(exist_ok=True)\n",
        "\n",
        "# Configure logging (only once)\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Function to check & install dependencies\n",
        "def install_package(package):\n",
        "    module_name = package.split('==')[0]\n",
        "    if importlib.util.find_spec(module_name) is None:\n",
        "        logger.info(f\"Installing {package} ...\")\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
        "    else:\n",
        "        logger.info(f\"{package} already installed.\")\n",
        "\n",
        "# Required packages\n",
        "packages = [\n",
        "    'requests', 'pandas', 'numpy==1.26.4', 'ta', 'ccxt',\n",
        "    'vectorbt', 'nltk', 'praw', 'gnews', 'newsapi-python',\n",
        "    'optuna', 'transformers', 'torch', 'tqdm', 'python-dotenv'\n",
        "]\n",
        "for pkg in packages:\n",
        "    install_package(pkg)\n",
        "\n",
        "# NLTK data setup\n",
        "import nltk\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "logger.info(\"Step 1 complete: Dependencies installed and folders ready.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lb1pOBhoC9SY"
      },
      "source": [
        "# Step 2: Market Data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oWCqFFPlg-lG"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# STEP 2: MARKET DATA FETCH (TOP 100)\n",
        "# =========================\n",
        "import ccxt\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "CACHE_FILE = Path(\"cache/top_pairs.csv\")\n",
        "\n",
        "def get_pairs(exchange, quote_currency=\"USDT\", min_volume=0):\n",
        "    \"\"\"Fetch tradable pairs from an exchange, filter by quote & volume.\"\"\"\n",
        "    try:\n",
        "        markets = exchange.load_markets()\n",
        "        tickers = exchange.fetch_tickers()\n",
        "        data = []\n",
        "\n",
        "        for symbol, ticker in tickers.items():\n",
        "            if symbol in markets:\n",
        "                if (quote_currency in symbol\n",
        "                    and markets[symbol].get('active', True)\n",
        "                    and markets[symbol].get('type', 'spot') == 'spot'):\n",
        "\n",
        "                    vol = ticker.get('quoteVolume', 0) or ticker.get('baseVolume', 0)\n",
        "                    try:\n",
        "                        vol = float(vol)\n",
        "                    except:\n",
        "                        vol = 0\n",
        "\n",
        "                    data.append({\n",
        "                        'symbol': symbol,\n",
        "                        'volume': vol,\n",
        "                        'exchange': exchange.id\n",
        "                    })\n",
        "\n",
        "        return pd.DataFrame(data)\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error fetching pairs from {exchange.id}: {e}\")\n",
        "        return pd.DataFrame(columns=[\"symbol\", \"volume\", \"exchange\"])\n",
        "\n",
        "\n",
        "def fetch_top_pairs(top_n=100):\n",
        "    logger.info(\"Fetching pairs from Bitget & KuCoin...\")\n",
        "\n",
        "    bitget = ccxt.bitget()\n",
        "    kucoin = ccxt.kucoin()\n",
        "\n",
        "    df_bitget = get_pairs(bitget)\n",
        "    df_kucoin = get_pairs(kucoin)\n",
        "\n",
        "    # Combine & sort by volume\n",
        "    df_all = pd.concat([df_bitget, df_kucoin], ignore_index=True)\n",
        "    df_all = df_all.sort_values(by=\"volume\", ascending=False).reset_index(drop=True)\n",
        "\n",
        "    # Take top N\n",
        "    df_top = df_all.head(top_n)\n",
        "\n",
        "    # Cache to CSV\n",
        "    df_top.to_csv(CACHE_FILE, index=False)\n",
        "    logger.info(f\"Saved top {len(df_top)} pairs to {CACHE_FILE}\")\n",
        "    return df_top\n",
        "\n",
        "# Run fetch step\n",
        "top_pairs_df = fetch_top_pairs(top_n=100)\n",
        "top_pairs_df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ViEcMkFVhlE8"
      },
      "source": [
        "#Technical Indicator Analysis\n",
        "This step will:\n",
        "\n",
        "Fetch live historical OHLCV data (candlestick data) from Bitget\n",
        "\n",
        "Calculate at least 4 key indicators:\n",
        "\n",
        "✅ RSI (Relative Strength Index)\n",
        "\n",
        "✅ MACD (Moving Average Convergence Divergence)\n",
        "\n",
        "✅ EMA (Exponential Moving Averages — 20, 50, 200)\n",
        "\n",
        "✅ Bollinger Bands\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c4PVyyqsMP6j"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FN1s0kY3hkOz"
      },
      "outputs": [],
      "source": [
        "# === Step 3 (v2) ===\n",
        "# Multi-timeframe TA with smart throttling + progressive caching (Colab-ready)\n",
        "import os\n",
        "import time\n",
        "import math\n",
        "import ccxt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from tqdm import tqdm\n",
        "import logging\n",
        "from datetime import datetime, timezone\n",
        "\n",
        "# CONFIG\n",
        "TIMEFRAMES = ['1h', '4h', '1d']\n",
        "OHLCV_LIMIT_FULL = 1000       # when no cache exists, fetch up to this many candles\n",
        "OHLCV_LIMIT_INCREMENT = 1000  # max candles to fetch when using 'since' (APIs often cap)\n",
        "MAX_WORKERS = 6               # total worker threads for parallel symbol processing\n",
        "RETRY_ATTEMPTS = 3\n",
        "RETRY_BACKOFF = 2             # exponential backoff base\n",
        "ADV_TA_DIR = Path(\"advanced_ta\")\n",
        "ADV_TA_DIR.mkdir(exist_ok=True, parents=True)\n",
        "MASTER_CSV = ADV_TA_DIR / \"master_multi_tf.csv\"\n",
        "TOP_PAIRS_CSV = Path(\"cache/top_pairs.csv\")  # from Step 2\n",
        "# Per-exchange delay (seconds) to avoid 429s — tune these if you still see limits\n",
        "EXCHANGE_DELAY = {\n",
        "    \"bitget\": 0.35,\n",
        "    \"kucoin\": 0.25\n",
        "}\n",
        "# Use conservative single-exchange concurrency (1 per exchange) to reduce 429s risk.\n",
        "PER_EXCHANGE_MAX_WORKERS = 2\n",
        "\n",
        "# Logging\n",
        "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
        "logger = logging.getLogger(\"step3_v2\")\n",
        "\n",
        "# Try TALIB\n",
        "try:\n",
        "    import talib\n",
        "    TALIB_AVAILABLE = True\n",
        "    logger.info(\"TA-Lib available.\")\n",
        "except Exception:\n",
        "    TALIB_AVAILABLE = False\n",
        "    logger.info(\"TA-Lib not available; using pandas fallbacks where needed.\")\n",
        "\n",
        "# Initialize CCXT clients with rate limit enabled\n",
        "EXCHANGE_CLIENTS = {\n",
        "    \"bitget\": ccxt.bitget({'enableRateLimit': True}),\n",
        "    \"kucoin\": ccxt.kucoin({'enableRateLimit': True}),\n",
        "}\n",
        "def get_exchange_client(exchange_id):\n",
        "    if exchange_id in EXCHANGE_CLIENTS:\n",
        "        return EXCHANGE_CLIENTS[exchange_id]\n",
        "    # default fallback: instantiate on-the-fly\n",
        "    return getattr(ccxt, exchange_id)({'enableRateLimit': True})\n",
        "\n",
        "# utility: convert ISO-like timeframe to ms using ccxt helper (safe fallback)\n",
        "def timeframe_to_ms(tf):\n",
        "    try:\n",
        "        return ccxt.Exchange().parse_timeframe(tf) * 1000\n",
        "    except Exception:\n",
        "        # fallback approximate mapping\n",
        "        if tf.endswith('h'):\n",
        "            return int(tf.replace('h', '')) * 60 * 60 * 1000\n",
        "        if tf.endswith('d'):\n",
        "            return int(tf.replace('d', '')) * 24 * 60 * 60 * 1000\n",
        "        return 60 * 1000\n",
        "\n",
        "# progressive OHLCV fetch: if cached, fetch from last_ts+1 else fetch full\n",
        "def fetch_ohlcv_incremental(exchange_id, symbol, timeframe, limit_full=OHLCV_LIMIT_FULL, limit_since=OHLCV_LIMIT_INCREMENT):\n",
        "    exchange = get_exchange_client(exchange_id)\n",
        "    out_path = ADV_TA_DIR / f\"{symbol.replace('/', '_')}_{timeframe}.csv\"\n",
        "    existing_df = None\n",
        "    since = None\n",
        "\n",
        "    # If cache exists, determine 'since' timestamp (ms)\n",
        "    if out_path.exists():\n",
        "        try:\n",
        "            existing_df = pd.read_csv(out_path, parse_dates=['timestamp']).set_index('timestamp')\n",
        "            if len(existing_df) > 0:\n",
        "                last_ts = existing_df.index[-1]\n",
        "                # convert to ms (UTC)\n",
        "                since = int(last_ts.tz_localize(None).timestamp() * 1000) + 1\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Couldn't read cache {out_path}: {e}\")\n",
        "            existing_df = None\n",
        "            since = None\n",
        "\n",
        "    # Define fetch function with retries\n",
        "    for attempt in range(1, RETRY_ATTEMPTS + 1):\n",
        "        try:\n",
        "            if since:\n",
        "                # fetch candles since last timestamp\n",
        "                ohlcv = exchange.fetch_ohlcv(symbol, timeframe=timeframe, since=since, limit=limit_since)\n",
        "            else:\n",
        "                ohlcv = exchange.fetch_ohlcv(symbol, timeframe=timeframe, limit=limit_full)\n",
        "            # convert to DataFrame\n",
        "            if not ohlcv:\n",
        "                # Some exchanges may return empty list if since too recent; that's OK\n",
        "                new_df = pd.DataFrame(columns=['open','high','low','close','volume'])\n",
        "            else:\n",
        "                df_new = pd.DataFrame(ohlcv, columns=['timestamp','open','high','low','close','volume'])\n",
        "                df_new['timestamp'] = pd.to_datetime(df_new['timestamp'], unit='ms')\n",
        "                df_new = df_new.set_index('timestamp')\n",
        "                # ensure numeric types\n",
        "                df_new[['open','high','low','close','volume']] = df_new[['open','high','low','close','volume']].apply(pd.to_numeric, errors='coerce')\n",
        "                new_df = df_new\n",
        "\n",
        "            # SAFE MERGE (avoid FutureWarning and handle empty frames)\n",
        "            if existing_df is None or existing_df.empty:\n",
        "                merged = new_df\n",
        "            elif new_df is None or new_df.empty:\n",
        "                merged = existing_df\n",
        "            else:\n",
        "                # Only append rows from new_df whose index is not already in existing_df\n",
        "                to_append = new_df[~new_df.index.isin(existing_df.index)]\n",
        "                if to_append.empty:\n",
        "                    merged = existing_df\n",
        "                else:\n",
        "                    merged = pd.concat([existing_df, to_append], axis=0)\n",
        "                    merged = merged[~merged.index.duplicated(keep='last')].sort_index()\n",
        "\n",
        "            return merged\n",
        "        except ccxt.BaseError as e:\n",
        "            wait = (RETRY_BACKOFF ** (attempt - 1))\n",
        "            logger.warning(f\"{exchange_id} fetch error for {symbol} {timeframe} attempt {attempt}/{RETRY_ATTEMPTS}: {e}. Backing off {wait}s\")\n",
        "            time.sleep(wait)\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Unexpected fetch error {exchange_id} {symbol} {timeframe}: {e}\")\n",
        "            break\n",
        "    # If all retries fail, return existing_df (could be None)\n",
        "    return existing_df\n",
        "\n",
        "# indicator computations (same as earlier; kept reasonably compact)\n",
        "def compute_indicators(df):\n",
        "    if df is None or df.empty:\n",
        "        return None\n",
        "    df = df.copy()\n",
        "    close = df['close']\n",
        "    high = df['high']\n",
        "    low = df['low']\n",
        "    vol = df['volume']\n",
        "\n",
        "    # EMA\n",
        "    df['ema50'] = close.ewm(span=50, adjust=False).mean()\n",
        "    df['ema200'] = close.ewm(span=200, adjust=False).mean()\n",
        "\n",
        "    # ATR\n",
        "    if TALIB_AVAILABLE:\n",
        "        df['atr'] = talib.ATR(high.values, low.values, close.values, timeperiod=14)\n",
        "    else:\n",
        "        tr1 = high - low\n",
        "        tr2 = (high - close.shift()).abs()\n",
        "        tr3 = (low - close.shift()).abs()\n",
        "        tr = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)\n",
        "        df['atr'] = tr.rolling(14).mean()\n",
        "\n",
        "    # RSI\n",
        "    if TALIB_AVAILABLE:\n",
        "        df['rsi'] = talib.RSI(close.values, timeperiod=14)\n",
        "    else:\n",
        "        delta = close.diff()\n",
        "        up = delta.clip(lower=0)\n",
        "        down = -1 * delta.clip(upper=0)\n",
        "        ma_up = up.ewm(com=13, adjust=False).mean()\n",
        "        ma_down = down.ewm(com=13, adjust=False).mean()\n",
        "        rs = ma_up / ma_down\n",
        "        df['rsi'] = 100 - (100 / (1 + rs))\n",
        "\n",
        "    # MACD\n",
        "    if TALIB_AVAILABLE:\n",
        "        macd, macdsig, macdhist = talib.MACD(close.values, fastperiod=12, slowperiod=26, signalperiod=9)\n",
        "        df['macd'] = macd\n",
        "        df['macd_signal'] = macdsig\n",
        "        df['macd_hist'] = macdhist\n",
        "    else:\n",
        "        ema12 = close.ewm(span=12, adjust=False).mean()\n",
        "        ema26 = close.ewm(span=26, adjust=False).mean()\n",
        "        macd = ema12 - ema26\n",
        "        signal = macd.ewm(span=9, adjust=False).mean()\n",
        "        df['macd'] = macd\n",
        "        df['macd_signal'] = signal\n",
        "        df['macd_hist'] = macd - signal\n",
        "\n",
        "    # Stochastic\n",
        "    low_min = low.rolling(14).min()\n",
        "    high_max = high.rolling(14).max()\n",
        "    df['stoch_k'] = ( (close - low_min) / (high_max - low_min) * 100 ).rolling(3).mean()\n",
        "    df['stoch_d'] = df['stoch_k'].rolling(3).mean()\n",
        "\n",
        "    # Bollinger Bands\n",
        "    ma20 = close.rolling(20).mean()\n",
        "    std20 = close.rolling(20).std()\n",
        "    df['bb_middle'] = ma20\n",
        "    df['bb_upper'] = ma20 + 2 * std20\n",
        "    df['bb_lower'] = ma20 - 2 * std20\n",
        "\n",
        "    # OBV\n",
        "    obv = [0]\n",
        "    c = close.values\n",
        "    v = vol.values\n",
        "    for i in range(1, len(c)):\n",
        "        if c[i] > c[i-1]:\n",
        "            obv.append(obv[-1] + (v[i] if not math.isnan(v[i]) else 0))\n",
        "        elif c[i] < c[i-1]:\n",
        "            obv.append(obv[-1] - (v[i] if not math.isnan(v[i]) else 0))\n",
        "        else:\n",
        "            obv.append(obv[-1])\n",
        "    df['obv'] = obv\n",
        "\n",
        "    # ADX\n",
        "    if TALIB_AVAILABLE:\n",
        "        df['adx'] = talib.ADX(high.values, low.values, close.values, timeperiod=14)\n",
        "    else:\n",
        "        up_move = high.diff()\n",
        "        down_move = -low.diff()\n",
        "        plus_dm = np.where((up_move > down_move) & (up_move > 0), up_move, 0.0)\n",
        "        minus_dm = np.where((down_move > up_move) & (down_move > 0), down_move, 0.0)\n",
        "        tr1 = high - low\n",
        "        tr2 = (high - close.shift()).abs()\n",
        "        tr3 = (low - close.shift()).abs()\n",
        "        tr = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)\n",
        "        atr14 = tr.rolling(14).mean()\n",
        "        plus_di = 100 * (pd.Series(plus_dm).rolling(14).mean() / atr14)\n",
        "        minus_di = 100 * (pd.Series(minus_dm).rolling(14).mean() / atr14)\n",
        "        dx = (abs(plus_di - minus_di) / (plus_di + minus_di)) * 100\n",
        "        df['adx'] = dx.rolling(14).mean()\n",
        "\n",
        "    # Ichimoku components\n",
        "    df['ichimoku_conv'] = ((high.rolling(9).max() + low.rolling(9).min()) / 2)\n",
        "    df['ichimoku_base'] = ((high.rolling(26).max() + low.rolling(26).min()) / 2)\n",
        "    df['ichimoku_span_a'] = ((df['ichimoku_conv'] + df['ichimoku_base']) / 2).shift(26)\n",
        "    df['ichimoku_span_b'] = ((high.rolling(52).max() + low.rolling(52).min()) / 2).shift(26)\n",
        "\n",
        "    return df\n",
        "\n",
        "# Fibonacci - compute from last `lookback` candles\n",
        "def fib_levels_from_df(df, lookback=100):\n",
        "    if df is None or df.empty:\n",
        "        return {}\n",
        "    recent = df[-lookback:]\n",
        "    swing_high = recent['high'].max()\n",
        "    swing_low = recent['low'].min()\n",
        "    diff = swing_high - swing_low\n",
        "    if diff == 0 or math.isnan(diff):\n",
        "        return {}\n",
        "    return {\n",
        "        'fib_0': swing_high,\n",
        "        'fib_0236': swing_high - 0.236 * diff,\n",
        "        'fib_0382': swing_high - 0.382 * diff,\n",
        "        'fib_05': swing_high - 0.5 * diff,\n",
        "        'fib_0618': swing_high - 0.618 * diff,\n",
        "        'fib_100': swing_low\n",
        "    }\n",
        "\n",
        "# worker for single symbol/timeframe: fetch incremental, compute indicators, save\n",
        "def process_symbol_tf(row, timeframe, per_exchange_delay=0.3):\n",
        "    symbol = row['symbol']\n",
        "    exchange_id = row.get('exchange', 'bitget')\n",
        "    out_path = ADV_TA_DIR / f\"{symbol.replace('/', '_')}_{timeframe}.csv\"\n",
        "\n",
        "    # fetch incremental ohlcv (merges with existing cache)\n",
        "    merged = fetch_ohlcv_incremental(exchange_id, symbol, timeframe)\n",
        "    if merged is None or merged.empty:\n",
        "        return {'symbol': symbol, 'timeframe': timeframe, 'status': 'no_data'}\n",
        "\n",
        "    # If the file existed, read old DF to compare length — skip compute if unchanged\n",
        "    try:\n",
        "        if out_path.exists():\n",
        "            old_df = pd.read_csv(out_path, parse_dates=['timestamp']).set_index('timestamp')\n",
        "        else:\n",
        "            old_df = None\n",
        "    except Exception:\n",
        "        old_df = None\n",
        "\n",
        "    # If merged equals old_df (no new candles), skip recomputation\n",
        "    if old_df is not None and len(merged) == len(old_df):\n",
        "        # still ensure we return a positive status (cached)\n",
        "        return {'symbol': symbol, 'timeframe': timeframe, 'status': 'cached', 'path': str(out_path)}\n",
        "\n",
        "    # compute indicators (we compute over full merged df to ensure indicator continuity)\n",
        "    df_with_ind = compute_indicators(merged)\n",
        "\n",
        "    # append fibonacci as scalar columns (last-known) for easier master merge\n",
        "    fib = fib_levels_from_df(merged, lookback=100)\n",
        "    for k, v in fib.items():\n",
        "        # fill column with same value for easier storage (useful when reading per-file)\n",
        "        df_with_ind[k] = v\n",
        "\n",
        "    # save the merged indicators back to CSV (overwrite)\n",
        "    try:\n",
        "        df_with_ind.reset_index().rename(columns={'index':'timestamp'}).to_csv(out_path, index=False)\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error saving {out_path}: {e}\")\n",
        "        return {'symbol': symbol, 'timeframe': timeframe, 'status': 'save_error'}\n",
        "\n",
        "    # throttle after each fetch to avoid 429s\n",
        "    delay = EXCHANGE_DELAY.get(exchange_id, per_exchange_delay)\n",
        "    time.sleep(delay)\n",
        "    return {'symbol': symbol, 'timeframe': timeframe, 'status': 'saved', 'path': str(out_path)}\n",
        "\n",
        "# orchestrator: group by exchange, process per-exchange in batches to respect delays\n",
        "def step3_process_all(top_pairs_df, timeframes=TIMEFRAMES, max_workers=MAX_WORKERS):\n",
        "    results = []\n",
        "    # group rows by exchange to process exchange-by-exchange\n",
        "    exchanges = top_pairs_df['exchange'].fillna('bitget').unique().tolist()\n",
        "    for exchange_id in exchanges:\n",
        "        logger.info(f\"Processing exchange {exchange_id} ...\")\n",
        "        rows = top_pairs_df[top_pairs_df['exchange'] == exchange_id].to_dict('records')\n",
        "        if not rows:\n",
        "            continue\n",
        "\n",
        "        # use a limited ThreadPool per exchange to avoid overwhelming it\n",
        "        per_exchange_workers = min(PER_EXCHANGE_MAX_WORKERS, max(1, max_workers // len(exchanges)))\n",
        "        with ThreadPoolExecutor(max_workers=per_exchange_workers) as ex:\n",
        "            futures = []\n",
        "            for row in rows:\n",
        "                for tf in timeframes:\n",
        "                    futures.append(ex.submit(process_symbol_tf, row, tf, EXCHANGE_DELAY.get(exchange_id, 0.3)))\n",
        "\n",
        "            for fut in tqdm(as_completed(futures), total=len(futures), desc=f\"{exchange_id} tasks\"):\n",
        "                try:\n",
        "                    r = fut.result()\n",
        "                    results.append(r)\n",
        "                except Exception as e:\n",
        "                    logger.error(f\"Worker exception: {e}\")\n",
        "    return results\n",
        "\n",
        "# Build master CSV by taking latest row per timeframe and prefixing\n",
        "def build_master_csv(top_pairs_df, timeframes=TIMEFRAMES):\n",
        "    rows = []\n",
        "    for _, row in top_pairs_df.iterrows():\n",
        "        symbol = row['symbol']\n",
        "        combined = {'symbol': symbol, 'exchange': row.get('exchange', '')}\n",
        "        for tf in timeframes:\n",
        "            path = ADV_TA_DIR / f\"{symbol.replace('/', '_')}_{tf}.csv\"\n",
        "            if not path.exists():\n",
        "                continue\n",
        "            df = pd.read_csv(path, parse_dates=['timestamp']).set_index('timestamp')\n",
        "            if df.empty:\n",
        "                continue\n",
        "            latest = df.iloc[-1].to_dict()\n",
        "            for k, v in latest.items():\n",
        "                combined[f\"{tf}_{k}\"] = v\n",
        "        rows.append(combined)\n",
        "    master_df = pd.DataFrame(rows)\n",
        "    master_df.to_csv(MASTER_CSV, index=False)\n",
        "    logger.info(f\"Master CSV written to {MASTER_CSV} ({len(master_df)} symbols).\")\n",
        "    return master_df\n",
        "\n",
        "# === RUN ===\n",
        "if not TOP_PAIRS_CSV.exists():\n",
        "    raise FileNotFoundError(f\"Expected {TOP_PAIRS_CSV} produced by Step 2. Run that first.\")\n",
        "\n",
        "top_pairs_df = pd.read_csv(TOP_PAIRS_CSV)\n",
        "top_pairs_df = top_pairs_df.head(100).reset_index(drop=True)\n",
        "\n",
        "logger.info(f\"Starting smart Step 3: {len(top_pairs_df)} symbols x {len(TIMEFRAMES)} timeframes.\")\n",
        "results = step3_process_all(top_pairs_df, timeframes=TIMEFRAMES, max_workers=MAX_WORKERS)\n",
        "logger.info(f\"Step 3 tasks completed: {len(results)} results.\")\n",
        "\n",
        "# create master file for LLM consumption\n",
        "master_df = build_master_csv(top_pairs_df, timeframes=TIMEFRAMES)\n",
        "logger.info(\"Step 3 (smart) complete.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpQDAtC6iL7Z"
      },
      "source": [
        "#News"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "74PM6olDiRoI"
      },
      "outputs": [],
      "source": [
        "!pip install snscrape\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oog3aCCuifnp"
      },
      "source": [
        "#News Analys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9B5z_2L1a1H5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['REDDIT_CLIENT_ID'] = 'Yae4rZFWI_VkZQbdGoRN-A'\n",
        "os.environ['REDDIT_CLIENT_SECRET'] = '8rU3QZMlYSOWTSrxUpUsq86pQkvOtA'\n",
        "os.environ['REDDIT_USER_AGENT'] = 'crypto_bot/1.0'\n",
        "os.environ['NEWSAPI_KEY'] = 'de2624f3e8eb46d38450387905a6db56'\n",
        "os.environ['CRYPTOPANIC_TOKEN'] = '500701eba889d6dc3a1140b41e2041d2f987a622'\n",
        "print(\"Env variables set in runtime.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wcvhKdzwa1Dm"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import requests\n",
        "import pandas as pd\n",
        "from transformers import pipeline\n",
        "import praw\n",
        "\n",
        "# ====== ENV / KEYS ======\n",
        "REDDIT_CLIENT_ID = 'Yae4rZFWI_VkZQbdGoRN-A'\n",
        "REDDIT_CLIENT_SECRET = '8rU3QZMlYSOWTSrxUpUsq86pQkvOtA'\n",
        "REDDIT_USER_AGENT = 'crypto_bot/1.0'\n",
        "NEWSAPI_KEY = 'de2624f3e8eb46d38450387905a6db56'\n",
        "CRYPTOPANIC_KEY = '500701eba889d6dc3a1140b41e2041d2f987a622'\n",
        "\n",
        "CACHE_DIR = \"news_sentiment\"\n",
        "os.makedirs(CACHE_DIR, exist_ok=True)\n",
        "\n",
        "# ====== Load FinBERT ======\n",
        "print(\"Loading FinBERT sentiment model...\")\n",
        "finbert = pipeline(\"sentiment-analysis\", model=\"ProsusAI/finbert\")\n",
        "\n",
        "# ====== CryptoPanic ======\n",
        "def fetch_cryptopanic():\n",
        "    url = (\n",
        "        f\"https://cryptopanic.com/api/v1/posts/\"\n",
        "        f\"?auth_token={CRYPTOPANIC_KEY}&kind=news&public=true\"\n",
        "    )\n",
        "    r = requests.get(url)\n",
        "    if r.status_code != 200:\n",
        "        print(\"CryptoPanic request failed:\", r.text)\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    data = r.json()\n",
        "    results = data.get(\"results\", [])\n",
        "    if not results:\n",
        "        print(\"CryptoPanic returned no results.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    rows = []\n",
        "    for post in results:\n",
        "        rows.append({\n",
        "            \"source\": \"CryptoPanic\",\n",
        "            \"title\": post[\"title\"],\n",
        "            \"url\": post[\"url\"],\n",
        "            \"publishedAt\": post.get(\"published_at\"),\n",
        "            \"sentiment\": finbert(post[\"title\"])[0][\"label\"]\n",
        "        })\n",
        "\n",
        "    df = pd.DataFrame(rows)\n",
        "    df.to_csv(f\"{CACHE_DIR}/cryptopanic_all.csv\", index=False)\n",
        "    print(f\"CryptoPanic: {len(df)} articles saved.\")\n",
        "    return df\n",
        "\n",
        "# ====== NewsAPI ======\n",
        "def fetch_newsapi():\n",
        "    url = (\n",
        "        f\"https://newsapi.org/v2/everything?q=crypto&language=en\"\n",
        "        f\"&sortBy=publishedAt&apiKey={NEWSAPI_KEY}\"\n",
        "    )\n",
        "    r = requests.get(url)\n",
        "    if r.status_code != 200:\n",
        "        print(\"NewsAPI request failed:\", r.text)\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    articles = r.json().get(\"articles\", [])\n",
        "    rows = []\n",
        "    for art in articles:\n",
        "        rows.append({\n",
        "            \"source\": \"NewsAPI\",\n",
        "            \"title\": art[\"title\"],\n",
        "            \"url\": art[\"url\"],\n",
        "            \"publishedAt\": art[\"publishedAt\"],\n",
        "            \"sentiment\": finbert(art[\"title\"])[0][\"label\"]\n",
        "        })\n",
        "\n",
        "    df = pd.DataFrame(rows)\n",
        "    df.to_csv(f\"{CACHE_DIR}/newsapi_all.csv\", index=False)\n",
        "    print(f\"NewsAPI: {len(df)} articles saved.\")\n",
        "    return df\n",
        "\n",
        "# ====== Reddit ======\n",
        "def fetch_reddit():\n",
        "    reddit = praw.Reddit(\n",
        "        client_id=REDDIT_CLIENT_ID,\n",
        "        client_secret=REDDIT_CLIENT_SECRET,\n",
        "        user_agent=REDDIT_USER_AGENT\n",
        "    )\n",
        "    subreddit = reddit.subreddit(\"CryptoCurrency+Bitcoin+Ethereum+CryptoMarkets\")\n",
        "    posts = subreddit.new(limit=50)\n",
        "    rows = []\n",
        "    for post in posts:\n",
        "        rows.append({\n",
        "            \"source\": \"Reddit\",\n",
        "            \"title\": post.title,\n",
        "            \"url\": f\"https://reddit.com{post.permalink}\",\n",
        "            \"publishedAt\": post.created_utc,\n",
        "            \"sentiment\": finbert(post.title)[0][\"label\"]\n",
        "        })\n",
        "\n",
        "    df = pd.DataFrame(rows)\n",
        "    df.to_csv(f\"{CACHE_DIR}/reddit_all.csv\", index=False)\n",
        "    print(f\"Reddit: {len(df)} posts saved.\")\n",
        "    return df\n",
        "\n",
        "# ====== Run All ======\n",
        "if __name__ == \"__main__\":\n",
        "    df_cp = fetch_cryptopanic()\n",
        "    df_na = fetch_newsapi()\n",
        "    df_rd = fetch_reddit()\n",
        "\n",
        "    # Merge all\n",
        "    all_df = pd.concat([df_cp, df_na, df_rd], ignore_index=True)\n",
        "    all_df.to_csv(f\"{CACHE_DIR}/all_sources.csv\", index=False)\n",
        "    print(f\"\\nTOTAL ARTICLES SAVED: {len(all_df)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--iru4wzndfw"
      },
      "source": [
        "#Addtional News"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JIoLI7W4nYqe"
      },
      "outputs": [],
      "source": [
        "import feedparser\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "def fetch_rss_news(feed_url, score_func=None, source_name=\"Unknown\"):\n",
        "    feed = feedparser.parse(feed_url)\n",
        "    news_list = []\n",
        "\n",
        "    for entry in feed.entries:\n",
        "        title = entry.get(\"title\", \"\").strip()\n",
        "        link = entry.get(\"link\", \"\").strip()\n",
        "        published = entry.get(\"published\", \"\").strip()\n",
        "\n",
        "        if not title:  # skip empty titles\n",
        "            continue\n",
        "\n",
        "        score = score_func(title) if score_func else None\n",
        "        news_list.append({\n",
        "            \"source\": source_name,\n",
        "            \"title\": title,\n",
        "            \"link\": link,\n",
        "            \"published\": published,\n",
        "            \"score\": score\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(news_list)\n",
        "\n",
        "\n",
        "def fetch_coindesk_rss(score_func=None):\n",
        "    return fetch_rss_news(\n",
        "        \"https://www.coindesk.com/arc/outboundfeeds/rss/\",\n",
        "        score_func,\n",
        "        source_name=\"Coindesk\"\n",
        "    )\n",
        "\n",
        "def fetch_cointelegraph_rss(score_func=None):\n",
        "    return fetch_rss_news(\n",
        "        \"https://cointelegraph.com/rss\",\n",
        "        score_func,\n",
        "        source_name=\"Cointelegraph\"\n",
        "    )\n",
        "\n",
        "\n",
        "# Example: Fetch news from both\n",
        "cd_df = fetch_coindesk_rss(score_func=None)\n",
        "ct_df = fetch_cointelegraph_rss(score_func=None)\n",
        "\n",
        "# Combine\n",
        "all_news_df = pd.concat([cd_df, ct_df], ignore_index=True)\n",
        "\n",
        "# Save to CSV\n",
        "timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "csv_path = f\"/content/news_sentiment/crypto_news_{timestamp}.csv\"\n",
        "all_news_df.to_csv(csv_path, index=False, encoding=\"utf-8\")\n",
        "\n",
        "print(f\"✅ Saved {len(all_news_df)} news articles to {csv_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uyf91meOuiYw"
      },
      "source": [
        "#DeepSeek LLM Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qL3JbX_o5pyr"
      },
      "outputs": [],
      "source": [
        "!pip install groq\n",
        "from groq import Groq\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gu798iCHEoah"
      },
      "outputs": [],
      "source": [
        "# In a Colab code cell (one-liners):\n",
        "%env GROQ_API_KEY=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\n",
        "%env TELEGRAM_BOT_TOKEN=8361038742:AAFE3wPyw0FZ-7QPLW8fetPYBAE19ZSLrjQ\n",
        "%env TELEGRAM_CHAT_ID=1083299833\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B_5Ty2d-rn9l"
      },
      "outputs": [],
      "source": [
        "# Cell 1: Data load + selection\n",
        "import os, json, math, time, hashlib, logging\n",
        "from pathlib import Path\n",
        "from datetime import datetime, timedelta\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
        "logger = logging.getLogger(\"signal_cell1\")\n",
        "\n",
        "# CONFIG (paths)\n",
        "MASTER_TA_PATH = Path(\"/content/advanced_ta/master_multi_tf.csv\")\n",
        "NEWS_DIR = Path(\"/content/news_sentiment\")\n",
        "DEBUG_OUT = Path(\"/content/selection_debug.csv\")\n",
        "\n",
        "# Tunables (can adjust)\n",
        "MIN_INDICATORS = 3\n",
        "REQUIRE_NEWS_AGREEMENT = False   # if True require news sign to match TA (looser by default)\n",
        "NEWS_ABS_MIN = 0.12              # minimum absolute news score to consider agreement\n",
        "TOP_K = 20                       # how many top candidates to present to LLM\n",
        "WEIGHT_SENTIMENT = 0.45\n",
        "WEIGHT_TA = 0.55\n",
        "\n",
        "# helpers\n",
        "def load_master(master_path=MASTER_TA_PATH):\n",
        "    if not master_path.exists():\n",
        "        raise FileNotFoundError(f\"Master TA file not found: {master_path}\")\n",
        "    df = pd.read_csv(master_path, dtype=str)\n",
        "    df.columns = [c.strip() for c in df.columns]\n",
        "    df['symbol'] = df['symbol'].astype(str).str.strip().str.upper()\n",
        "    logger.info(\"Loaded master TA: %d rows, %d symbols\", len(df), df['symbol'].nunique())\n",
        "    return df\n",
        "\n",
        "def load_sentiment_map(news_dir=NEWS_DIR, symbols_list=None):\n",
        "    \"\"\"Aggregate any CSVs under news_dir. tries common score columns.\n",
        "       Returns dict symbol->avg_score. If symbol inference fails, returns 'UNKNOWN' bucket.\n",
        "    \"\"\"\n",
        "    files = list(news_dir.rglob(\"*.csv\"))\n",
        "    rows = []\n",
        "    score_cols = ['score','sentiment_score','compound','sentiment','avg_score']\n",
        "    for f in files:\n",
        "        try:\n",
        "            df = pd.read_csv(f)\n",
        "        except Exception as e:\n",
        "            logger.warning(\"Skipping unreadable news file %s: %s\", f, e)\n",
        "            continue\n",
        "        # infer symbol from filename if possible: tokens like BTC, ETH, ADA etc\n",
        "        fname = f.name.upper()\n",
        "        inferred = None\n",
        "        if symbols_list:\n",
        "            for s in symbols_list:\n",
        "                token = s.split(\"/\")[0].replace(\"USDT\",\"\")\n",
        "                if token in fname or token in fname.replace(\"_\",\"\"):\n",
        "                    inferred = s\n",
        "                    break\n",
        "        # find a numeric score column\n",
        "        sc = None\n",
        "        for c in score_cols:\n",
        "            if c in df.columns:\n",
        "                sc = c; break\n",
        "        # if sentiment text (positive/negative), map it\n",
        "        for _, r in df.iterrows():\n",
        "            try:\n",
        "                if sc:\n",
        "                    raw = r.get(sc, 0.0)\n",
        "                    # convert strings like 'positive' or 'neg' to numeric\n",
        "                    if isinstance(raw, str):\n",
        "                        v = raw.lower()\n",
        "                        if 'pos' in v: v = 1.0\n",
        "                        elif 'neg' in v: v = -1.0\n",
        "                        elif '%' in v:\n",
        "                            try: v = float(v.replace('%',''))/100.0\n",
        "                            except: v = 0.0\n",
        "                        else:\n",
        "                            try: v = float(v)\n",
        "                            except: v = 0.0\n",
        "                    else:\n",
        "                        v = float(raw) if not (pd.isna(raw)) else 0.0\n",
        "                else:\n",
        "                    # try 'title' + 'description' heuristic (fallback neutral)\n",
        "                    v = 0.0\n",
        "                rows.append({\"symbol\": inferred or \"UNKNOWN\", \"score\": float(v)})\n",
        "            except Exception:\n",
        "                continue\n",
        "    if not rows:\n",
        "        logger.warning(\"No news rows found in %s\", news_dir)\n",
        "        return {s:0.0 for s in (symbols_list or [])}\n",
        "    nd = pd.DataFrame(rows)\n",
        "    grouped = nd.groupby('symbol')['score'].mean().to_dict()\n",
        "    # build full map for all symbols_list\n",
        "    sentiment_map = {}\n",
        "    if symbols_list:\n",
        "        for s in symbols_list:\n",
        "            sentiment_map[s] = float(grouped.get(s, 0.0))\n",
        "    # include unknown bucket\n",
        "    if 'UNKNOWN' in grouped:\n",
        "        sentiment_map['UNKNOWN'] = float(grouped['UNKNOWN'])\n",
        "    # include any other symbols found\n",
        "    for k,v in grouped.items():\n",
        "        if k not in sentiment_map:\n",
        "            sentiment_map[k] = float(v)\n",
        "    logger.info(\"Built sentiment_map for %d symbols (includes UNKNOWN=%s)\", len(sentiment_map), sentiment_map.get('UNKNOWN',None))\n",
        "    return sentiment_map\n",
        "\n",
        "# indicator helpers (adapted from earlier system)\n",
        "def _get_first_numeric(row, candidates):\n",
        "    for c in candidates:\n",
        "        if c in row and row[c] not in (None,\"\",\"nan\",\"NaN\"):\n",
        "            try: return float(row[c])\n",
        "            except: pass\n",
        "    return None\n",
        "\n",
        "def indicator_signs_from_row(row):\n",
        "    def first(pref): return _get_first_numeric(row, pref)\n",
        "    signs={}\n",
        "    ema50 = first(['1d_ema50','4h_ema50','1h_ema50']); ema200 = first(['1d_ema200','4h_ema200','1h_ema200'])\n",
        "    signs['ema'] = 1 if (ema50 is not None and ema200 is not None and ema50>ema200) else (-1 if (ema50 is not None and ema200 is not None and ema50<ema200) else 0)\n",
        "    macd = first(['1d_macd','4h_macd','1h_macd']); macd_sig = first(['1d_macd_signal','4h_macd_signal','1h_macd_signal'])\n",
        "    signs['macd'] = 1 if (macd is not None and macd_sig is not None and macd>macd_sig) else (-1 if (macd is not None and macd_sig is not None and macd<macd_sig) else 0)\n",
        "    macdh = first(['1d_macd_hist','4h_macd_hist','1h_macd_hist']); signs['macd_hist'] = 1 if (macdh is not None and macdh>0) else (-1 if (macdh is not None and macdh<0) else 0)\n",
        "    rsi = first(['1d_rsi','4h_rsi','1h_rsi']); signs['rsi'] = 1 if (rsi is not None and rsi<35) else (-1 if (rsi is not None and rsi>65) else 0)\n",
        "    stoch_k = first(['1d_stoch_k','4h_stoch_k','1h_stoch_k']); stoch_d = first(['1d_stoch_d','4h_stoch_d','1h_stoch_d'])\n",
        "    signs['stoch'] = 1 if (stoch_k is not None and stoch_d is not None and stoch_k>stoch_d) else (-1 if (stoch_k is not None and stoch_d is not None and stoch_k<stoch_d) else 0)\n",
        "    close = first(['1d_close','4h_close','1h_close']); bb_upper = first(['1d_bb_upper','4h_bb_upper','1h_bb_upper']); bb_lower = first(['1d_bb_lower','4h_bb_lower','1h_bb_lower'])\n",
        "    signs['bb'] = 1 if (close is not None and bb_lower is not None and close < bb_lower) else (-1 if (close is not None and bb_upper is not None and close > bb_upper) else 0)\n",
        "    adx = first(['1d_adx','4h_adx','1h_adx']); signs['adx'] = (signs.get('ema') or signs.get('macd')) if (adx is not None and adx>=30) else 0\n",
        "    span_a = first(['1d_ichimoku_span_a','4h_ichimoku_span_a','1h_ichimoku_span_a']); span_b = first(['1d_ichimoku_span_b','4h_ichimoku_span_b','1h_ichimoku_span_b'])\n",
        "    signs['ichimoku'] = 1 if (span_a is not None and span_b is not None and span_a>span_b) else (-1 if (span_a is not None and span_b is not None and span_a<span_b) else 0)\n",
        "    fib0618 = first(['1d_fib_0618','4h_fib_0618','1h_fib_0618']); fib0236 = first(['1d_fib_0236','4h_fib_0236','1h_fib_0236'])\n",
        "    signs['fib'] = 1 if (close is not None and fib0618 is not None and close>fib0618) else (-1 if (close is not None and fib0236 is not None and close<fib0236) else 0)\n",
        "    return signs\n",
        "\n",
        "def ta_score_from_row(row):\n",
        "    signs = indicator_signs_from_row(row)\n",
        "    weights = {'ema':1.0,'macd':1.0,'macd_hist':0.6,'rsi':0.9,'stoch':0.6,'bb':0.6,'adx':0.5,'ichimoku':0.7,'fib':0.4}\n",
        "    total_w = sum(weights.values())\n",
        "    ssum = sum(signs.get(k,0)*w for k,w in weights.items())\n",
        "    return float(ssum/total_w) if total_w else 0.0\n",
        "\n",
        "def select_candidates_by_rule(master_df, sentiment_map, min_indicators=MIN_INDICATORS, news_abs_min=NEWS_ABS_MIN, require_news_agreement=REQUIRE_NEWS_AGREEMENT, top_k=TOP_K):\n",
        "    rows=[]\n",
        "    for _, r in master_df.iterrows():\n",
        "        sym = r['symbol']\n",
        "        signs = indicator_signs_from_row(r)\n",
        "        indicator_count = sum(1 for v in signs.values() if v!=0)\n",
        "        if indicator_count < min_indicators:\n",
        "            continue\n",
        "        direction = 1 if sum(signs.values())>0 else (-1 if sum(signs.values())<0 else 0)\n",
        "        sent = float(np.clip(sentiment_map.get(sym, sentiment_map.get(\"UNKNOWN\",0.0)), -1, 1))\n",
        "        news_sign = 1 if sent>0 else (-1 if sent<0 else 0)\n",
        "        if require_news_agreement and news_sign != 0:\n",
        "            if news_sign != direction and abs(sent) < news_abs_min:\n",
        "                continue\n",
        "        ta_s = ta_score_from_row(r)\n",
        "        composite = WEIGHT_SENTIMENT*sent + WEIGHT_TA*ta_s\n",
        "        # small boost for indicator_count (diminishing)\n",
        "        composite_adj = composite * (1 + min(indicator_count,10)/40.0)\n",
        "        rows.append({\"symbol\":sym,\"indicator_count\":indicator_count,\"direction\":direction,\"sentiment\":sent,\"ta\":ta_s,\"composite\":composite_adj,\"signs\":signs})\n",
        "    if not rows:\n",
        "        return []\n",
        "    dfc = pd.DataFrame(rows)\n",
        "    dfc['abs_composite'] = dfc['composite'].abs()\n",
        "    dfc = dfc.sort_values(['abs_composite','indicator_count'], ascending=[False,False])\n",
        "    out = dfc.head(top_k).to_dict(orient='records')\n",
        "    # debug save\n",
        "    try:\n",
        "        pd.DataFrame(rows).sort_values(['abs_composite','indicator_count'], ascending=[False,False]).to_csv(DEBUG_OUT, index=False)\n",
        "        logger.info(\"Wrote %s for inspection\", DEBUG_OUT)\n",
        "    except Exception:\n",
        "        pass\n",
        "    return out\n",
        "\n",
        "# Run selection now and expose variables for next cell\n",
        "master = load_master()\n",
        "symbols = sorted(master['symbol'].unique().tolist())\n",
        "sentiment_map = load_sentiment_map(NEWS_DIR, symbols_list=symbols)\n",
        "candidates = select_candidates_by_rule(master, sentiment_map)\n",
        "logger.info(\"Candidates selected: %d\", len(candidates))\n",
        "# Show top summary\n",
        "for c in candidates[:20]:\n",
        "    print(f\"{c['symbol']:12} indicators={c['indicator_count']:2d} composite={c['composite']:.3f} sent={c['sentiment']:.3f} ta={c['ta']:.3f}\")\n",
        "# Keep 'candidates' in notebook namespace for cell 2\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4e_sFguUHaXC"
      },
      "source": [
        "#LLM and prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7NQEXYOntEbm"
      },
      "outputs": [],
      "source": [
        "# ===== Ready-to-paste: Groq-hybrid signaler (sanitizes cache + strict JSON prompt) =====\n",
        "# BEFORE running: either set environment variables or paste your keys in the placeholders below.\n",
        "# Recommended (Colab): run\n",
        "#   %env GROQ_API_KEY=your_key_here\n",
        "#   %env TELEGRAM_BOT_TOKEN=your_bot_token_here\n",
        "#   %env TELEGRAM_CHAT_ID=@your_channel_or_id\n",
        "#\n",
        "# If you prefer to hardcode (not recommended), set the values in the CONFIG section.\n",
        "\n",
        "import os, time, json, re, random, logging, hashlib\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "import requests\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
        "logger = logging.getLogger(\"groq_hybrid_ready\")\n",
        "\n",
        "# ---------------- CONFIG ----------------\n",
        "ROOT = Path(\"/content\")\n",
        "MASTER_TA_PATH = ROOT / \"advanced_ta\" / \"master_multi_tf.csv\"\n",
        "NEWS_DIR = ROOT / \"news_sentiment\"\n",
        "LLM_CACHE_PATH = NEWS_DIR / \"llm_cache_groq.json\"\n",
        "NEWS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "if not LLM_CACHE_PATH.exists():\n",
        "    LLM_CACHE_PATH.write_text(json.dumps({}))\n",
        "\n",
        "# Put keys here if you want to embed them (NOT recommended). Otherwise set them via env vars:\n",
        "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\", \"\")          # set via %env GROQ_API_KEY=...\n",
        "GROQ_MODEL = os.getenv(\"GROQ_MODEL\", \"deepseek-r1-distill-llama-70b\")\n",
        "TELEGRAM_BOT_TOKEN = os.getenv(\"TELEGRAM_BOT_TOKEN\", \"\")  # set via %env TELEGRAM_BOT_TOKEN=...\n",
        "TELEGRAM_CHAT_ID   = os.getenv(\"TELEGRAM_CHAT_ID\", \"\")    # set via %env TELEGRAM_CHAT_ID=@channel_or_id\n",
        "\n",
        "# Tunables\n",
        "MIN_INDICATORS = 3\n",
        "NEWS_ABS_MIN = 0.12\n",
        "REQUIRE_NEWS_AGREEMENT = False\n",
        "TOP_K = 8\n",
        "WEIGHT_SENTIMENT = 0.45\n",
        "WEIGHT_TA = 0.55\n",
        "MIN_CONFIDENCE_TO_SEND = 45\n",
        "DRY_RUN = True\n",
        "\n",
        "# LLM retry/backoff\n",
        "LLM_MAX_ATTEMPTS = 4\n",
        "LLM_BASE_BACKOFF = 1.6\n",
        "LLM_JITTER = 0.6\n",
        "\n",
        "# ---------------- Utilities ----------------\n",
        "def sanitize_llm_cache(path: Path):\n",
        "    \"\"\"Keep only JSON arrays in cache values (store parsed lists). Backup old cache.\"\"\"\n",
        "    if not path.exists():\n",
        "        path.write_text(json.dumps({}))\n",
        "        return\n",
        "    try:\n",
        "        raw = path.read_text()\n",
        "        cache = json.loads(raw)\n",
        "    except Exception as e:\n",
        "        logger.warning(\"Cache not valid JSON, backing up and resetting: %s\", e)\n",
        "        bak = path.with_suffix(\".bak.json\")\n",
        "        path.rename(bak)\n",
        "        path.write_text(json.dumps({}))\n",
        "        return\n",
        "\n",
        "    cleaned = {}\n",
        "    dropped = []\n",
        "    for k, v in cache.items():\n",
        "        if isinstance(v, list):\n",
        "            cleaned[k] = v\n",
        "            continue\n",
        "        if not isinstance(v, str):\n",
        "            dropped.append(k); continue\n",
        "        s = v.strip()\n",
        "        # Try parse directly\n",
        "        try:\n",
        "            parsed = json.loads(s)\n",
        "            if isinstance(parsed, list):\n",
        "                cleaned[k] = parsed\n",
        "                continue\n",
        "        except Exception:\n",
        "            pass\n",
        "        # attempt to extract first JSON array\n",
        "        m = re.search(r'(\\[\\s*\\{.*\\}\\s*\\])', s, flags=re.DOTALL)\n",
        "        if m:\n",
        "            try:\n",
        "                parsed = json.loads(m.group(1))\n",
        "                if isinstance(parsed, list):\n",
        "                    cleaned[k] = parsed; continue\n",
        "            except Exception:\n",
        "                pass\n",
        "        # fallback: substring from first [ to last ]\n",
        "        try:\n",
        "            i = s.index('[')\n",
        "            j = s.rindex(']')\n",
        "            candidate = s[i:j+1]\n",
        "            parsed = json.loads(candidate)\n",
        "            if isinstance(parsed, list):\n",
        "                cleaned[k] = parsed; continue\n",
        "        except Exception:\n",
        "            dropped.append(k)\n",
        "            continue\n",
        "    # backup and write cleaned\n",
        "    bak = path.with_suffix(\".bak.json\")\n",
        "    path.rename(bak)\n",
        "    path.write_text(json.dumps(cleaned, indent=2))\n",
        "    logger.info(\"Sanitized LLM cache. Kept %d entries, dropped %d. Backup at %s\", len(cleaned), len(dropped), bak)\n",
        "\n",
        "def _load_llm_cache():\n",
        "    try:\n",
        "        return json.loads(LLM_CACHE_PATH.read_text())\n",
        "    except Exception:\n",
        "        return {}\n",
        "def _save_llm_cache(c):\n",
        "    try:\n",
        "        LLM_CACHE_PATH.write_text(json.dumps(c))\n",
        "    except Exception as e:\n",
        "        logger.warning(\"Failed to write llm cache: %s\", e)\n",
        "\n",
        "# ---------------- Data loaders / TA scoring (adapted from your logic) ----------------\n",
        "def load_master(master_path=MASTER_TA_PATH):\n",
        "    if not master_path.exists():\n",
        "        raise FileNotFoundError(f\"Master TA file not found: {master_path}\")\n",
        "    df = pd.read_csv(master_path, dtype=str)\n",
        "    df['symbol'] = df['symbol'].astype(str).str.strip().str.upper()\n",
        "    return df\n",
        "\n",
        "def load_sentiment_map(news_dir=NEWS_DIR, symbols_list=None):\n",
        "    files = list(news_dir.rglob(\"*.csv\"))\n",
        "    rows=[]\n",
        "    for f in files:\n",
        "        try:\n",
        "            df = pd.read_csv(f)\n",
        "        except Exception:\n",
        "            continue\n",
        "        score_col=None\n",
        "        for c in ['score','sentiment','compound','sentiment_score']:\n",
        "            if c in df.columns:\n",
        "                score_col=c; break\n",
        "        for _, r in df.iterrows():\n",
        "            s = 0.0\n",
        "            if score_col and score_col in df.columns:\n",
        "                try: s = float(r.get(score_col,0.0))\n",
        "                except: s = 0.0\n",
        "            assigned=None\n",
        "            if symbols_list:\n",
        "                fname = f.name.upper()\n",
        "                for sym in symbols_list:\n",
        "                    if sym.replace('/','_') in fname or sym.replace('/','') in fname:\n",
        "                        assigned=sym; break\n",
        "            rows.append({\"symbol\":assigned or \"UNKNOWN\", \"score\": s})\n",
        "    if not rows:\n",
        "        return {s:0.0 for s in (symbols_list or [])}\n",
        "    nd = pd.DataFrame(rows)\n",
        "    grouped = nd.groupby('symbol')['score'].mean().to_dict()\n",
        "    sentiment_map = {sym: float(grouped.get(sym,0.0)) for sym in (symbols_list or grouped.keys())}\n",
        "    if 'UNKNOWN' in grouped: sentiment_map['UNKNOWN']=float(grouped['UNKNOWN'])\n",
        "    return sentiment_map\n",
        "\n",
        "def _get_first_numeric(row, candidates):\n",
        "    for c in candidates:\n",
        "        if c in row and row[c] not in (None,\"\",\"nan\",\"NaN\"):\n",
        "            try: return float(row[c])\n",
        "            except: pass\n",
        "    return None\n",
        "\n",
        "def indicator_signs_from_row(row):\n",
        "    def first(pref): return _get_first_numeric(row, pref)\n",
        "    signs={}\n",
        "    ema50 = first(['1d_ema50','4h_ema50','1h_ema50']); ema200 = first(['1d_ema200','4h_ema200','1h_ema200'])\n",
        "    signs['ema'] = 1 if (ema50 is not None and ema200 is not None and ema50>ema200) else (-1 if (ema50 is not None and ema200 is not None and ema50<ema200) else 0)\n",
        "    macd = first(['1d_macd','4h_macd','1h_macd']); macd_sig = first(['1d_macd_signal','4h_macd_signal','1h_macd_signal'])\n",
        "    signs['macd'] = 1 if (macd is not None and macd_sig is not None and macd>macd_sig) else (-1 if (macd is not None and macd_sig is not None and macd<macd_sig) else 0)\n",
        "    macdh = first(['1d_macd_hist','4h_macd_hist','1h_macd_hist']); signs['macd_hist'] = 1 if (macdh is not None and macdh>0) else (-1 if (macdh is not None and macdh<0) else 0)\n",
        "    rsi = first(['1d_rsi','4h_rsi','1h_rsi']); signs['rsi'] = 1 if (rsi is not None and rsi<35) else (-1 if (rsi is not None and rsi>65) else 0)\n",
        "    stoch_k = first(['1d_stoch_k','4h_stoch_k','1h_stoch_k']); stoch_d = first(['1d_stoch_d','4h_stoch_d','1h_stoch_d'])\n",
        "    signs['stoch'] = 1 if (stoch_k is not None and stoch_d is not None and stoch_k>stoch_d) else (-1 if (stoch_k is not None and stoch_d is not None and stoch_k<stoch_d) else 0)\n",
        "    close = first(['1d_close','4h_close','1h_close']); bb_upper = first(['1d_bb_upper','4h_bb_upper','1h_bb_upper']); bb_lower = first(['1d_bb_lower','4h_bb_lower','1h_bb_lower'])\n",
        "    signs['bb'] = 1 if (close is not None and bb_lower is not None and close < bb_lower) else (-1 if (close is not None and bb_upper is not None and close > bb_upper) else 0)\n",
        "    adx = first(['1d_adx','4h_adx','1h_adx']); signs['adx'] = (signs.get('ema') or signs.get('macd')) if (adx is not None and adx>=30) else 0\n",
        "    span_a = first(['1d_ichimoku_span_a','4h_ichimoku_span_a','1h_ichimoku_span_a']); span_b = first(['1d_ichimoku_span_b','4h_ichimoku_span_b','1h_ichimoku_span_b'])\n",
        "    signs['ichimoku'] = 1 if (span_a is not None and span_b is not None and span_a>span_b) else (-1 if (span_a is not None and span_b is not None and span_a<span_b) else 0)\n",
        "    fib0618 = first(['1d_fib_0618','4h_fib_0618','1h_fib_0618']); fib0236 = first(['1d_fib_0236','4h_fib_0236','1h_fib_0236'])\n",
        "    signs['fib'] = 1 if (close is not None and fib0618 is not None and close>fib0618) else (-1 if (close is not None and fib0236 is not None and close<fib0236) else 0)\n",
        "    return signs\n",
        "\n",
        "def ta_score_from_row(row):\n",
        "    signs = indicator_signs_from_row(row)\n",
        "    weights = {'ema':1.0,'macd':1.0,'macd_hist':0.6,'rsi':0.9,'stoch':0.6,'bb':0.6,'adx':0.5,'ichimoku':0.7,'fib':0.4}\n",
        "    total_w = sum(weights.values()); ssum = sum(signs.get(k,0)*w for k,w in weights.items())\n",
        "    return float(ssum/total_w) if total_w else 0.0\n",
        "\n",
        "def select_candidates_by_rule(master_df, sentiment_map, min_indicators=MIN_INDICATORS, news_abs_min=NEWS_ABS_MIN, require_news_agreement=REQUIRE_NEWS_AGREEMENT, top_k=TOP_K):\n",
        "    rows=[]\n",
        "    for _, r in master_df.iterrows():\n",
        "        sym = r['symbol']\n",
        "        signs = indicator_signs_from_row(r)\n",
        "        indicator_count = sum(1 for v in signs.values() if v!=0)\n",
        "        if indicator_count < min_indicators: continue\n",
        "        direction = 1 if sum(signs.values())>0 else (-1 if sum(signs.values())<0 else 0)\n",
        "        sent = float(np.clip(sentiment_map.get(sym,0.0), -1, 1))\n",
        "        news_sign = 1 if sent>0 else (-1 if sent<0 else 0)\n",
        "        if require_news_agreement and news_sign != 0:\n",
        "            if news_sign != direction and abs(sent) < news_abs_min: continue\n",
        "        ta_s = ta_score_from_row(r)\n",
        "        composite = WEIGHT_SENTIMENT*sent + WEIGHT_TA*ta_s\n",
        "        # Optionally include sample headlines count if available (we don't require here)\n",
        "        rows.append({\"symbol\":sym,\"indicator_count\":indicator_count,\"direction\":direction,\"sentiment\":sent,\"ta\":ta_s,\"composite\":composite,\"signs\":signs})\n",
        "    if not rows: return []\n",
        "    dfc = pd.DataFrame(rows); dfc['abs_composite'] = dfc['composite'].abs()\n",
        "    dfc = dfc.sort_values(['abs_composite','indicator_count'], ascending=[False,False])\n",
        "    return dfc.head(top_k).to_dict(orient='records')\n",
        "\n",
        "# ---------------- LLM helpers ----------------\n",
        "def extract_json_array_from_model(content):\n",
        "    if not content: return None\n",
        "    s = content.strip()\n",
        "    # If starts with [ and valid JSON, parse directly\n",
        "    try:\n",
        "        if s.startswith('['):\n",
        "            parsed = json.loads(s)\n",
        "            if isinstance(parsed, list): return parsed\n",
        "    except Exception:\n",
        "        pass\n",
        "    # find first JSON array\n",
        "    m = re.search(r'(\\[\\s*\\{.*?\\}\\s*\\])', content, flags=re.DOTALL)\n",
        "    if m:\n",
        "        try:\n",
        "            parsed = json.loads(m.group(1))\n",
        "            if isinstance(parsed, list): return parsed\n",
        "        except Exception:\n",
        "            pass\n",
        "    # fallback from first [ to last ]\n",
        "    try:\n",
        "        i = content.index('[')\n",
        "        j = content.rindex(']')\n",
        "        candidate = content[i:j+1]\n",
        "        parsed = json.loads(candidate)\n",
        "        if isinstance(parsed, list): return parsed\n",
        "    except Exception:\n",
        "            return None\n",
        "    return None\n",
        "\n",
        "def call_groq(prompt, model=GROQ_MODEL, max_attempts=LLM_MAX_ATTEMPTS):\n",
        "    key = GROQ_API_KEY\n",
        "    # try python client\n",
        "    try:\n",
        "        from groq import Groq\n",
        "        client = Groq(api_key=key) if key else Groq()\n",
        "        attempt = 0\n",
        "        while attempt < max_attempts:\n",
        "            attempt += 1\n",
        "            try:\n",
        "                completion = client.chat.completions.create(model=model, messages=[{\"role\":\"user\",\"content\":prompt}], timeout=30)\n",
        "                return completion.choices[0].message.content\n",
        "            except Exception as e:\n",
        "                backoff = (LLM_BASE_BACKOFF ** attempt) + random.random()*LLM_JITTER\n",
        "                logger.warning(\"Groq client attempt %d failed: %s. Backoff %.1fs\", attempt, e, backoff)\n",
        "                time.sleep(backoff)\n",
        "        logger.error(\"Groq client failed after %d attempts\", max_attempts)\n",
        "    except Exception as e:\n",
        "        logger.info(\"groq python client not available or failed to init (%s). Falling back to HTTP\", e)\n",
        "\n",
        "    # HTTP fallback\n",
        "    if not key:\n",
        "        logger.warning(\"No GROQ_API_KEY set; skipping LLM call.\")\n",
        "        return None\n",
        "    url = \"https://api.groq.com/openai/v1/chat/completions\"\n",
        "    payload = {\"model\": model, \"messages\":[{\"role\":\"user\",\"content\":prompt}], \"temperature\":0.0, \"max_tokens\":512}\n",
        "    attempt=0\n",
        "    while attempt < max_attempts:\n",
        "        attempt += 1\n",
        "        try:\n",
        "            r = requests.post(url, headers={\"Authorization\": f\"Bearer {key}\"}, json=payload, timeout=30)\n",
        "            r.raise_for_status()\n",
        "            data = r.json()\n",
        "            return data.get(\"choices\",[{}])[0].get(\"message\",{}).get(\"content\")\n",
        "        except Exception as e:\n",
        "            backoff = (LLM_BASE_BACKOFF ** attempt) + random.random()*LLM_JITTER\n",
        "            logger.warning(\"Groq HTTP attempt %d failed: %s. Backoff %.1fs\", attempt, e, backoff)\n",
        "            time.sleep(backoff)\n",
        "    logger.error(\"Groq HTTP failed after %d attempts.\", max_attempts)\n",
        "    return None\n",
        "\n",
        "def deterministic_fallback(selected_candidates, top_n=1):\n",
        "    if not selected_candidates: return []\n",
        "    sel = sorted(selected_candidates, key=lambda x:(abs(x['composite']), x['indicator_count']), reverse=True)\n",
        "    out=[]\n",
        "    for cand in sel[:top_n]:\n",
        "        action = \"BUY\" if cand['composite']>0 else (\"SELL\" if cand['composite']<0 else \"HOLD\")\n",
        "        conf = max(5, min(99, int(round(abs(cand['composite'])*100))))\n",
        "        reason = f\"Rule fallback: {cand['indicator_count']} indicators; composite {cand['composite']:.2f}; sentiment {cand['sentiment']:.2f}.\"\n",
        "        out.append({\"symbol\": cand['symbol'], \"action\": action, \"confidence\": conf, \"reason\": reason})\n",
        "    return out\n",
        "\n",
        "def send_telegram_message(text):\n",
        "    if DRY_RUN:\n",
        "        logger.info(\"(dry) Would send Telegram message:\\n%s\", text)\n",
        "        return True\n",
        "    if not TELEGRAM_BOT_TOKEN or not TELEGRAM_CHAT_ID:\n",
        "        logger.warning(\"Telegram not configured. Not sending.\")\n",
        "        return False\n",
        "    url = f\"https://api.telegram.org/bot{TELEGRAM_BOT_TOKEN}/sendMessage\"\n",
        "    try:\n",
        "        r = requests.post(url, json={\"chat_id\": TELEGRAM_CHAT_ID, \"text\": text}, timeout=15)\n",
        "        r.raise_for_status()\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        logger.warning(\"Telegram send failed: %s\", e)\n",
        "        return False\n",
        "\n",
        "# ---------------- Orchestration ----------------\n",
        "def run_once(dry_run=DRY_RUN, top_n=3):\n",
        "    # sanitize cache first\n",
        "    sanitize_llm_cache(LLM_CACHE_PATH)\n",
        "\n",
        "    master = load_master()\n",
        "    symbols = sorted(master['symbol'].unique().tolist())\n",
        "    sentiment_map = load_sentiment_map(NEWS_DIR, symbols_list=symbols)\n",
        "    selected = select_candidates_by_rule(master, sentiment_map, top_k=TOP_K)\n",
        "    logger.info(\"Selected %d candidates by rule\", len(selected))\n",
        "    if not selected:\n",
        "        logger.info(\"No candidates found.\")\n",
        "        return []\n",
        "\n",
        "    # caching key (deterministic)\n",
        "    key_json = json.dumps(selected, sort_keys=True, default=str)\n",
        "    key_hash = hashlib.sha256(key_json.encode()).hexdigest()\n",
        "    cache = _load_llm_cache()\n",
        "    if key_hash in cache:\n",
        "        logger.info(\"LLM cache hit.\")\n",
        "        refined = cache[key_hash]\n",
        "    else:\n",
        "        # build strict prompt (system + user style)\n",
        "        PROMPT = (\n",
        "            \"SYSTEM:\\n\"\n",
        "            \"You are a conservative, factual, professional crypto trading analyst assistant with 20 years experience and an expert in technnical and fundamental analysis.\\n\"\n",
        "            \"You MUST OUTPUT EXACTLY a single JSON ARRAY and NOTHING ELSE. No commentary, no analysis, no internal chains of thought.\\n\\n\"\n",
        "            \"USER:\\n\"\n",
        "             \"use your deethinking mode and reasoning to execute the given task.\"\n",
        "            \"You are given candidate trades (JSON list). For each candidate choose up to {TOP_N} best trades to ACT on now (24-72h horizon).\\n\"\n",
        "            \"Output format EXACTLY (example):\\n\"\n",
        "            '[{\"symbol\":\"BTC/USDT\",\"action\":\"BUY\",\"confidence\":0-100,\"reason\":\"one-sentence rationale\"}]\\n\\n'\n",
        "            \"Scoring rules (apply heuristics):\\n\"\n",
        "            \"- base_conf = round(abs(composite) * 100)\\n\"\n",
        "            \"- +10 if indicator_count >= 6\\n\"\n",
        "            \"- +10 if there are >=2 supporting headlines (if available)\\n\"\n",
        "            \"- -30 if sentiment and TA direction conflict and abs(sentiment) > 0.2\\n\"\n",
        "            \"- -20 if indicator_count < 3\\n\"\n",
        "            \"- Entry Price – the ideal buy/sell level.\"\n",
        "              \"Analyze the provided market data and generate a trading signal.\"\n",
        "              \"Respond with:\"\n",
        "\n",
        "              \"Entry Price – the ideal buy/sell level\"\n",
        "\n",
        "              \"Take Profit (TP) – realistic profit target.\"\n",
        "\n",
        "              \"Stop Loss (SL) – risk protection level.\"\n",
        "\n",
        "              \"Short Explanation – 1-2 sentences summarizing why this signal is valid, based on market trends, indicators, or news.\"\n",
        "              \"Use concise, clear formatting. Example:\"\n",
        "              \"Entry: 27500 USDT\"\n",
        "              \"TP: 28200 USDT\"\n",
        "              \"SL: 27200 USDT\"\n",
        "            \"- -20 if indicator_count < 3\\n\"\n",
        "            \"Cap confidence 1..99. If confidence < {MIN_CONF}, prefer 'HOLD'.\\n\"\n",
        "            \"Be concise. Output EXACTLY a JSON array.\\n\\n\"\n",
        "            \"Candidates:\\n\"\n",
        "        ).replace(\"{TOP_N}\", str(top_n)).replace(\"{MIN_CONF}\", str(MIN_CONFIDENCE_TO_SEND))\n",
        "        PROMPT = PROMPT + json.dumps(selected, indent=2)\n",
        "        # call LLM\n",
        "        content = call_groq(PROMPT, model=GROQ_MODEL)\n",
        "        parsed = extract_json_array_from_model(content) if content else None\n",
        "        if parsed is None:\n",
        "            logger.info(\"LLM produced no parseable JSON; using deterministic fallback.\")\n",
        "            parsed = deterministic_fallback(selected, top_n=1)\n",
        "        # cache parsed list\n",
        "        cache[key_hash] = parsed\n",
        "        _save_llm_cache(cache)\n",
        "        refined = parsed\n",
        "\n",
        "    # validate & send top results\n",
        "    sent = []\n",
        "    # refined should be a list\n",
        "    if not isinstance(refined, list):\n",
        "        logger.warning(\"Refined result not a list; applying fallback.\")\n",
        "        refined = deterministic_fallback(selected, top_n=1)\n",
        "\n",
        "    for sig in refined[:top_n]:\n",
        "        if not isinstance(sig, dict): continue\n",
        "        sym = sig.get('symbol'); action = sig.get('action'); conf = int(sig.get('confidence',0)); reason = sig.get('reason','')\n",
        "        if sym not in symbols:\n",
        "            logger.warning(\"Unknown symbol suggested: %s\", sym); continue\n",
        "        if conf < MIN_CONFIDENCE_TO_SEND:\n",
        "            logger.info(\"Skipping %s due to low confidence %d\", sym, conf); continue\n",
        "        if action not in (\"BUY\",\"SELL\",\"HOLD\"):\n",
        "            logger.warning(\"Invalid action %s for %s\", action, sym); continue\n",
        "        msg = f\"🚀 Top Signal:\\nPair: {sym}\\nSignal: {action}\\nConfidence: {conf}%\\nReason: {reason}\"\n",
        "        ok = send_telegram_message(msg)\n",
        "        sent.append({\"symbol\":sym,\"action\":action,\"confidence\":conf,\"reason\":reason,\"sent\":ok})\n",
        "    # log decisions locally\n",
        "    try:\n",
        "        DECISIONS_DIR = ROOT / \"decisions\"; DECISIONS_DIR.mkdir(exist_ok=True)\n",
        "        p = DECISIONS_DIR / \"decisions_log.csv\"\n",
        "        dfrow = {\"timestamp\": datetime.utcnow().isoformat(), \"selected\": json.dumps(selected), \"refined\": json.dumps(refined), \"signals\": json.dumps(sent)}\n",
        "        df = pd.DataFrame([dfrow])\n",
        "        if p.exists(): df.to_csv(p, index=False, mode='a', header=False)\n",
        "        else: df.to_csv(p, index=False)\n",
        "    except Exception as e:\n",
        "        logger.warning(\"Failed to write decisions log: %s\", e)\n",
        "\n",
        "    logger.info(\"Run complete. Sent signals: %d\", len(sent))\n",
        "    return sent\n",
        "\n",
        "# ---------------- Quick run (dry-run by default) ----------------\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"DRY_RUN =\", DRY_RUN, \"MIN_CONFIDENCE_TO_SEND =\", MIN_CONFIDENCE_TO_SEND)\n",
        "    try:\n",
        "        results = run_once(dry_run=DRY_RUN, top_n=3)\n",
        "        print(\"Done. Signals:\", results)\n",
        "    except Exception as e:\n",
        "        logger.exception(\"Error during run: %s\", e)\n",
        "        raise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cy1086EeHuQS"
      },
      "source": [
        "#TELEGRAM SETUP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b5nggmKjzsSS"
      },
      "outputs": [],
      "source": [
        "# === Send top cached LLM signals to Telegram (or dry-run) ===\n",
        "import os, json, requests\n",
        "from pathlib import Path\n",
        "from urllib.parse import quote_plus\n",
        "\n",
        "# CONFIG - set via %env in Colab OR place values here (not recommended)\n",
        "NEWS_DIR = Path(\"/content/news_sentiment\")\n",
        "LLM_CACHE_PATH = NEWS_DIR / \"llm_cache_groq.json\"\n",
        "\n",
        "# Put your keys in Colab env with:\n",
        "#   %env TELEGRAM_BOT_TOKEN=8361038742:AAFE3wPy...\n",
        "#   %env TELEGRAM_CHAT_ID=@YourChannelOrId\n",
        "TELEGRAM_BOT_TOKEN = os.getenv(\"TELEGRAM_BOT_TOKEN\", \"8361038742:AAFE3wPyw0FZ-7QPLW8fetPYBAE19ZSLrjQ\")\n",
        "TELEGRAM_CHAT_ID = os.getenv(\"TELEGRAM_CHAT_ID\", \"@Musty021\")\n",
        "\n",
        "# Tunables\n",
        "DRY_RUN = False        # True = only print what WOULD be sent. Set False to actually send.\n",
        "TOP_N = 3             # how many top signals to send (by confidence)\n",
        "MIN_CONF = 25         # minimum confidence to actually send (or show)\n",
        "\n",
        "# Safety check - cache exists?\n",
        "if not LLM_CACHE_PATH.exists():\n",
        "    raise FileNotFoundError(f\"LLM cache not found at: {LLM_CACHE_PATH}\")\n",
        "\n",
        "# Load cache\n",
        "raw = LLM_CACHE_PATH.read_text()\n",
        "try:\n",
        "    cache = json.loads(raw)\n",
        "except Exception as e:\n",
        "    raise RuntimeError(f\"Failed to parse JSON cache: {e}\")\n",
        "\n",
        "# Flatten all cached lists into a single list\n",
        "signals = []\n",
        "for k, v in cache.items():\n",
        "    if not isinstance(v, list):\n",
        "        # try to parse if string\n",
        "        if isinstance(v, str):\n",
        "            try:\n",
        "                parsed = json.loads(v)\n",
        "                if isinstance(parsed, list):\n",
        "                    v = parsed\n",
        "                else:\n",
        "                    continue\n",
        "            except Exception:\n",
        "                continue\n",
        "        else:\n",
        "            continue\n",
        "    for entry in v:\n",
        "        if not isinstance(entry, dict):\n",
        "            continue\n",
        "        # normalize keys\n",
        "        sym = entry.get(\"symbol\") or entry.get(\"pair\") or entry.get(\"ticker\")\n",
        "        action = (entry.get(\"action\") or entry.get(\"signal\") or \"\").upper()\n",
        "        conf = entry.get(\"confidence\")\n",
        "        reason = entry.get(\"reason\",\"\")\n",
        "        try:\n",
        "            conf = int(round(float(conf)))\n",
        "        except Exception:\n",
        "            conf = 0\n",
        "        if not sym or action not in (\"BUY\",\"SELL\",\"HOLD\"):\n",
        "            continue\n",
        "        signals.append({\"symbol\": sym, \"action\": action, \"confidence\": conf, \"reason\": reason, \"source_key\": k})\n",
        "\n",
        "if not signals:\n",
        "    print(\"No valid signals found in cache.\")\n",
        "else:\n",
        "    # dedupe by symbol (keep highest confidence per symbol)\n",
        "    by_symbol = {}\n",
        "    for s in signals:\n",
        "        sym = s['symbol']\n",
        "        if sym not in by_symbol or s['confidence'] > by_symbol[sym]['confidence']:\n",
        "            by_symbol[sym] = s\n",
        "    unique_signals = list(by_symbol.values())\n",
        "    # sort by confidence desc\n",
        "    unique_signals.sort(key=lambda x: x['confidence'], reverse=True)\n",
        "    # filter by MIN_CONF\n",
        "    filtered = [s for s in unique_signals if s['confidence'] >= MIN_CONF]\n",
        "    top = filtered[:TOP_N]\n",
        "\n",
        "    if not top:\n",
        "        print(f\"No signals meet MIN_CONF = {MIN_CONF}. Available candidates (top 10 shown):\")\n",
        "        for s in unique_signals[:10]:\n",
        "            print(s)\n",
        "    else:\n",
        "        print(f\"Top {len(top)} signals to send (DRY_RUN={DRY_RUN}):\")\n",
        "        for s in top:\n",
        "            print(s)\n",
        "\n",
        "        # Send (or dry-run)\n",
        "        def send_via_telegram(text):\n",
        "            if DRY_RUN:\n",
        "                print(\"(dry) would send:\", text)\n",
        "                return True\n",
        "            if not TELEGRAM_BOT_TOKEN or not TELEGRAM_CHAT_ID:\n",
        "                print(\"Telegram credentials missing. Set TELEGRAM_BOT_TOKEN and TELEGRAM_CHAT_ID as environment variables.\")\n",
        "                return False\n",
        "            url = f\"https://api.telegram.org/bot{TELEGRAM_BOT_TOKEN}/sendMessage\"\n",
        "            payload = {\"chat_id\": TELEGRAM_CHAT_ID, \"text\": text}\n",
        "            try:\n",
        "                r = requests.post(url, json=payload, timeout=15)\n",
        "                r.raise_for_status()\n",
        "                return True\n",
        "            except Exception as e:\n",
        "                print(\"Telegram send failed:\", e)\n",
        "                return False\n",
        "\n",
        "        results = []\n",
        "        for s in top:\n",
        "            msg = f\"🚀 Top Signal:\\nPair: {s['symbol']}\\nSignal: {s['action']}\\nConfidence: {s['confidence']}%\\nReason: {s['reason']}\"\n",
        "            ok = send_via_telegram(msg)\n",
        "            results.append({**s, \"sent\": ok})\n",
        "\n",
        "        print(\"Send results:\", results)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}